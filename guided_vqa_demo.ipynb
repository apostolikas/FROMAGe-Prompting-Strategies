{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook is used to give you a demonstration of how visual question-answering is performed using FROMAGe. Specifically, it aims to prove how in-context learning is applied for vqa using the guided-vqa dataset and how a visual augmentation does not always mean that it an augmentation of the prompt will increase the model's performance for a downstream task. The image below describes the procedure in a more comprehensible way.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images_report/gvqa.png\" width=\"1400\" height=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fromage import models\n",
    "import json \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, CLIPSegForImageSegmentation, OneFormerProcessor, OneFormerForUniversalSegmentation, AutoTokenizer, AutoModel\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the FROMAGe model used in the paper.\n",
    "model_dir = './fromage_model/'\n",
    "model = models.load_fromage(model_dir)\n",
    "\n",
    "# Load the first model for segmentation of the query image (CLIPSeg)\n",
    "processor_clip = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "vis_model_clip = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "\n",
    "# Load the second model for segmentation of the query image (Oneformer)\n",
    "processor_oneformer = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n",
    "vis_model_oneformer = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n",
    "\n",
    "# Load the language model to extract text embeddings (all-MiniLM-L6-v2)\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "lm = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./guided_vqa/guided_vqa_shots_1_ways_2_all_questions.json', 'r') as f:\n",
    "    vqa_data = json.load(f)\n",
    "\n",
    "vqa_sublist = vqa_data[20:25]\n",
    "vqa_images_folder = './guided_vqa'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions\n",
    "\n",
    "- The display_interleaved_outputs function is used to plot the images generated by the model.\n",
    "- The clipseg_segment_image and the oneformer_segment image are the function described in the figure of the Overview section. Specifically they refer to the middle and right part of the figure.\n",
    "- The compute_score function computes the cosine similarity of the text embeddings of the output of the model vs the answer (i.e. label) to determine how close or not is the model to the right answer of the question. The figure below explains in a simple way the aforementioned procedure.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images_report/embeds_cos_sim.png\" width=\"700\" height=\"200\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_interleaved_outputs(model_outputs, one_img_per_ret=True):\n",
    "    for output in model_outputs:\n",
    "        if type(output) == str:\n",
    "            print(output)\n",
    "        elif type(output) == list:\n",
    "            if one_img_per_ret:\n",
    "                plt.figure(figsize=(3, 3))\n",
    "                plt.imshow(np.array(output[0]))\n",
    "            else:\n",
    "                fig, ax = plt.subplots(1, len(output), figsize=(3 * len(output), 3))\n",
    "                for i, image in enumerate(output):\n",
    "                    image = np.array(image)\n",
    "                    ax[i].imshow(image)\n",
    "                    ax[i].set_title(f'Retrieval #{i+1}')\n",
    "            plt.show()\n",
    "        elif type(output) == Image.Image:\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.imshow(np.array(output))\n",
    "            plt.show()\n",
    "\n",
    "def cos_sim(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "\n",
    "def mean_pooling(model_output: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    token_embeddings = model_output[0] \n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def clipseg_segment_image(question_image):\n",
    "    encoded_image = processor_clip(images=[question_image], return_tensors = 'pt')\n",
    "    outputs = vis_model_clip(**encoded_image, conditional_pixel_values = encoded_image.pixel_values)\n",
    "    segmented_image = outputs.logits.unsqueeze(0)\n",
    "    segmented_pil_image = to_pil_image(segmented_image).resize((224, 224)).convert('RGB')\n",
    "    return segmented_pil_image\n",
    "    \n",
    "\n",
    "def oneformer_segment_image(question_image):\n",
    "    # Semantic Segmentation\n",
    "    semantic_inputs = processor_oneformer(images=question_image, task_inputs=[\"semantic\"], return_tensors=\"pt\")\n",
    "    semantic_outputs = vis_model_oneformer(**semantic_inputs)\n",
    "    predicted_semantic_map = processor_oneformer.post_process_semantic_segmentation(semantic_outputs, target_sizes=[question_image.size[::-1]])[0]\n",
    "    semantic_map = to_pil_image(predicted_semantic_map.float())\n",
    "    # Instance Segmentation\n",
    "    instance_inputs = processor_oneformer(images=question_image, task_inputs=[\"instance\"], return_tensors=\"pt\")\n",
    "    instance_outputs = vis_model_oneformer(**instance_inputs)\n",
    "    predicted_instance_map = processor_oneformer.post_process_instance_segmentation(instance_outputs, target_sizes=[question_image.size[::-1]])[0][\"segmentation\"]\n",
    "    instance_map = to_pil_image(predicted_instance_map.float())\n",
    "    # Panoptic Segmentation\n",
    "    panoptic_inputs = processor_oneformer(images=question_image, task_inputs=[\"panoptic\"], return_tensors=\"pt\")\n",
    "    panoptic_outputs = vis_model_oneformer(**panoptic_inputs)\n",
    "    predicted_panoptic_map = processor_oneformer.post_process_panoptic_segmentation(panoptic_outputs, target_sizes=[question_image.size[::-1]])[0][\"segmentation\"]\n",
    "    panoptic_map = to_pil_image(predicted_panoptic_map.float())\n",
    "    \n",
    "    return semantic_map, instance_map, panoptic_map\n",
    "\n",
    "def compute_score(model_outputs_original, model_outputs_clip_segment,model_outputs_oneformer_segment,answer):\n",
    "    # Tokenize the input\n",
    "    encoded_unaugmented_input = tokenizer(model_outputs_original, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_augmented_clip_input = tokenizer(model_outputs_clip_segment, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_augmented_oneformer_input = tokenizer(model_outputs_oneformer_segment, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_target_input = tokenizer(answer, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # FF through the model\n",
    "    with torch.no_grad():\n",
    "        model_unaugmented_output = lm(**encoded_unaugmented_input)\n",
    "        model_augmented_clip_output = lm(**encoded_augmented_clip_input)\n",
    "        model_augmented_oneformer_output = lm(**encoded_augmented_oneformer_input)\n",
    "        model_target_output = lm(**encoded_target_input)\n",
    "\n",
    "    # Process the embeddings\n",
    "    unaugmented_embeddings = F.normalize(mean_pooling(model_unaugmented_output, encoded_unaugmented_input['attention_mask']), p=2, dim=1)\n",
    "    augmented_clip_embeddings = F.normalize(mean_pooling(model_augmented_clip_output, encoded_augmented_clip_input['attention_mask']), p=2, dim=1)\n",
    "    augmented_oneformer_embeddings = F.normalize(mean_pooling(model_augmented_oneformer_output, encoded_augmented_oneformer_input['attention_mask']), p=2, dim=1)\n",
    "    target_embeddings = F.normalize(mean_pooling(model_target_output, encoded_target_input['attention_mask']), p=2, dim=1)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    augmented_clip_score = cos_sim(augmented_clip_embeddings, target_embeddings)\n",
    "    augmented_onerformer_score = cos_sim(augmented_oneformer_embeddings, target_embeddings)\n",
    "    unaugmented_score = cos_sim(unaugmented_embeddings, target_embeddings)\n",
    "\n",
    "    return augmented_clip_score, augmented_onerformer_score, unaugmented_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We obviously can not pass all the data through the model, because it needs time. So we will just pick 3 examples for the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vqa_dict in vqa_sublist:\n",
    "    \n",
    "    image1_path = vqa_dict['image_1']\n",
    "    image1 = Image.open(os.path.join(vqa_images_folder,image1_path)).resize((224, 224)).convert('RGB')\n",
    "    caption1 = vqa_dict['caption_1']\n",
    "    display_interleaved_outputs(image1)\n",
    "    print(caption1)\n",
    "\n",
    "    image2_path = vqa_dict['image_2']\n",
    "    image2 = Image.open(os.path.join(vqa_images_folder,image2_path)).resize((224, 224)).convert('RGB')\n",
    "    caption2 = vqa_dict['caption_2']\n",
    "    display_interleaved_outputs(image2)\n",
    "    print(caption2)\n",
    "\n",
    "    question_image_path = vqa_dict['question_image']\n",
    "    question_image = Image.open(os.path.join(vqa_images_folder,question_image_path)).resize((224, 224)).convert('RGB')\n",
    "    question = vqa_dict['question']\n",
    "    answer = vqa_dict['answer']\n",
    "    display_interleaved_outputs(question_image)\n",
    "    print(question)\n",
    "\n",
    "    # ClipSeg - segment query image \n",
    "    segmented_pil_image = clipseg_segment_image(question_image)\n",
    "\n",
    "    # Oneformer - segment query image\n",
    "    semantic_map, instance_map, panoptic_map = oneformer_segment_image(question_image)\n",
    "\n",
    "    # Generate output using the original query image\n",
    "    model_input_original = [ \n",
    "                image1, caption1, \n",
    "                image2, caption2, \n",
    "                question_image, \n",
    "                'Q: ' + question]\n",
    "    model_outputs_original = model.generate_for_images_and_texts(model_input_original, num_words=15, max_num_rets=0)\n",
    "\n",
    "    # Generate output using visual augmented prompt (Oneformer)\n",
    "    model_input_oneformer_segment = [ \n",
    "                image1, caption1, \n",
    "                image2, caption2, \n",
    "                semantic_map, instance_map, panoptic_map, \n",
    "                question_image, \n",
    "                'Q: ' + question]\n",
    "    model_outputs_oneformer_segment = model.generate_for_images_and_texts(model_input_oneformer_segment, num_words=15, max_num_rets=0)\n",
    "\n",
    "    # Generate output using visual augmented prompt (CLIPSeg)\n",
    "    model_input_clip_segment = [ \n",
    "                image1, caption1, \n",
    "                image2, caption2, \n",
    "                segmented_pil_image, question_image, \n",
    "                'Q: ' + question]\n",
    "    model_outputs_clip_segment = model.generate_for_images_and_texts(model_input_clip_segment, num_words=15, max_num_rets=0)\n",
    "\n",
    "    # Compute the scores by comparing the text embeddings\n",
    "    augmented_clip_score, augmented_onerformer_score, unaugmented_score = compute_score(model_outputs_original, model_outputs_clip_segment,model_outputs_oneformer_segment,answer)\n",
    "\n",
    "    print(\"The question is :\", question, \" and the answer is :\" ,answer)\n",
    "    print(\"Cos sim between original output - answer :\" ,unaugmented_score)\n",
    "    print(\"Cos sim between output using CLIPSeg - answer\" ,augmented_clip_score)\n",
    "    print(\"Cos sim between output using Oneformer - answer\" ,augmented_onerformer_score)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
