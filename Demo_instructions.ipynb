{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook is used to give you a demonstration of how to run each experiment using FROMAGe. Specifically, it aims to prove how in-context learning is applied for several tasks different types of prompting strategies (e.g. text or visual augmentations) can increase the model's performance for each downstream task.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image Captioning\n",
    "\n",
    "### Important note\n",
    "\n",
    "Make sure you download the data as stated in the github readme.md and place them in the main directory of the project (when you clone the repo).\n",
    "\n",
    "### Some useful functions\n",
    "- The cos_sim computes function the cosine similarity. We need it to compare the text embeddings of the outputs of the model.\n",
    "- The mean_pooling function is used as an processing tool for the embeddings.\n",
    "- The compare_embeddings function uses the Mini-LM-L6 model to generate the text embeddings and then calls the mean_pooling function and the cos_sim function to provide a final score. The following image describes the aforementioned procedure.\n",
    "\n",
    "### Instructions :\n",
    "\n",
    "If you execute image_captioning_demo.py through jobs/image_captioning_demo.job you will get the the results in a few minutes and what essentialy happens in the main function is :\n",
    "1. Load the model\n",
    "2. Load some samples from the dataset\n",
    "3. Run the inference loop\n",
    "\n",
    "There are comments in the image_captioning_demo.py file for each section.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images_report/Visual_augmentation_of_prompt.png\" width=\"920\" height=\"280\" />\n",
    "</p>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images_report/embeds_cos_sim.png\" width=\"700\" height=\"200\" />\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image Retrieval from Text (using the Flickr dataset)\n",
    "\n",
    "This task requires the same data as above so once you run the above experiment, you are good to go in this one too.\n",
    "\n",
    "What happens in the image_retrieval_demo.py file is pretty much the same as above, but in this case we evaluate whether text augmentation can help the model perform better.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images_report/Text_augmentation_of_prompt.png\" width=\"720\" height=\"300\" />\n",
    "</p>\n",
    "\n",
    "The comparison of the two generated image to the target image is being done using the cosine similarity of the visual embeddings extracted from CLIP from the FROMAGe for the images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Guided VQA\n",
    "\n",
    "### Important note\n",
    "\n",
    "Make sure you download the data as stated in the github readme.md and place them in the main directory of the project (when you clone the repo).\n",
    "\n",
    "What happens in the guided_vqa_demo.py file is that we check whether visual augmentation (image segmentation) of the prompt helps the model perform better or not.\n",
    "\n",
    "&nbsp;\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images_report/gvqa.png\" width=\"1400\" height=\"400\" />\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Video Captioning\n",
    "\n",
    "Write here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Image Classification\n",
    "\n",
    "Write here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visual Dialog \n",
    "\n",
    "With this task, we aim to prove how compressing complex text input (in this case in the form of a caption and a dialog) into a more clear and compact manner leads to improved capability of retrieving an image that suits the context. The image below describes the procedure in a more comprehensible way.\n",
    "\n",
    "### Important note\n",
    "\n",
    "Make sure you download the data as stated in the github readme.md and place them in the main directory of the project (when you clone the repo).\n",
    "\n",
    "### Some useful functions\n",
    "- The load_dialogs function loads the data.\n",
    "- The gpt_prompt function generates the new captions using text-davinci-003.\n",
    "- The get_image function shows an image by giving the image id.\n",
    "- The get_prompt_list function creates prompts from dataframe\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"images_report/visualdialog_scheme.png\" alt=\"Image\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2_gpu",
   "language": "python",
   "name": "dl2_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
