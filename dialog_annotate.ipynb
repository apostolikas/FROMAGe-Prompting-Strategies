{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from visual_dialog import dialog_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 1\n",
    "num_qa = 5\n",
    "# ret_img = True\n",
    "include_Q_A = False\n",
    "gpt_prompt = False\n",
    "\n",
    "# path = \"visual_dialog/1_examples_5_qa_ret_img_nocontext.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_annotations_folder = './visual_dialog/annotations'\n",
    "\n",
    "# experiment_name = f'{num_examples}_examples_{num_qa}_qa_{\"ret_img\" if ret_img else \"ret_text\"}_{\"gpt_prompt\" if gpt_prompt else \"nocontext\"}'\n",
    "experiment_name = f'{num_examples}_examples_{num_qa}_qa_{\"inc_QA\" if include_Q_A else \"only_caption\"}{\"_GPT\" if gpt_prompt else \"\"}'\n",
    "\n",
    "baseline_experiment = f'{num_examples}_examples_{num_qa}_qa_{\"inc_QA\" if include_Q_A else \"only_caption\"}'\n",
    "\n",
    "dialog_annotations_csv_path = f'{dialog_annotations_folder}/{experiment_name}.csv'\n",
    "\n",
    "if os.path.exists(dialog_annotations_csv_path):\n",
    "    annotations_df = pd.read_csv(dialog_annotations_csv_path, encoding='utf8', \n",
    "                                 dtype={'id': 'int', 'prediction': 'int'})\n",
    "else:\n",
    "    if not os.path.exists(dialog_annotations_folder):\n",
    "        os.makedirs(dialog_annotations_folder)\n",
    "    annotations_df = pd.DataFrame({ \n",
    "        'id': pd.Series(dtype='int'), 'prediction': pd.Series(dtype='int') })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scan experiment folder without recursion\n",
    "data_path = f'./visual_dialog/{experiment_name}.json'\n",
    "baseline_path = f'./visual_dialog/{baseline_experiment}.json'\n",
    "baseline_prompts = None\n",
    "\n",
    "with open(data_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "if data_path != baseline_path:\n",
    "    with open(baseline_path, 'r') as file:\n",
    "        baseline_data = json.load(file)\n",
    "    baseline_prompts = baseline_data['prompts']\n",
    "\n",
    "# Retrieve the lists\n",
    "prompts = data['prompts']\n",
    "outputs = data['outputs']\n",
    "\n",
    "print(f\"Amount of samples:\", len(prompts), \"\\n\")\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    # skip annotated story\n",
    "    if i in annotations_df['id'].values:\n",
    "        continue\n",
    "    # if baseline_prompts != None:\n",
    "    if baseline_prompts:\n",
    "        print(\"Unaugmented text input:\")\n",
    "        dialog_utils.display_prompt(baseline_prompts[i])\n",
    "        print(\"\\nGPT augmented text input\\n\")\n",
    "    \n",
    "    dialog_utils.display_prompt(prompts[i])\n",
    "    print('\\n', '=' * 50, '\\n')\n",
    "    dialog_utils.display_output(outputs[i])\n",
    "\n",
    "    prediction = ''\n",
    "    while (prediction != '0') and (prediction != '0.5') and (prediction != '1') and (prediction != '2'):\n",
    "        prediction = input('prediction: 0 (false) - 0.5 (partly correct) - 1 (correct) - 2 (stop)')\n",
    "    \n",
    "    if prediction == '2':\n",
    "        clear_output(wait=False)\n",
    "        break\n",
    "\n",
    "    annotations_df.loc[len(annotations_df)] = [i, float(prediction)]\n",
    "\n",
    "    clear_output(wait=False)\n",
    "\n",
    "annotations_df.to_csv(dialog_annotations_csv_path, encoding='utf8', index=False)\n",
    "\n",
    "print('\\n', 'Done', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = f\"visual_dialog/{num_examples}_examples_{num_qa}_qa_{'ret_img' if ret_img == True else 'ret_text'}{'_nocontext' if provide_context == False else ''}.json\"\n",
    "\n",
    "# Read the JSON strings back from the file\n",
    "# with open(path, 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Retrieve the lists\n",
    "# prompts = data['prompts']\n",
    "# outputs = data['outputs']\n",
    "\n",
    "# print(len(prompts))\n",
    "# print(len(outputs))\n",
    "\n",
    "# # Print the retrieved lists\n",
    "# if len(prompts) == len(outputs):\n",
    "#     print('correct lengths')\n",
    "# else:\n",
    "#     print('incorrect lengths')\n",
    "\n",
    "# for i in range(len(prompts)):\n",
    "#     dialog_utils.display_prompt(prompts[i])\n",
    "#     print(\"####################OUTPUT###########################\")\n",
    "#     dialog_utils.display_output(outputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
