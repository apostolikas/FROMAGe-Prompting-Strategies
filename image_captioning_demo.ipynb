{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook is used to give you a demonstration of how image captioning is performed using FROMAGe. Specifically, it aims to prove how in-context learning is applied for image captioning using the Flickr-8k dataset and how a visual augmentation can increase the model's performance for this downstream task.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images_report/Visual_augmentation_of_prompt.png\" width=\"920\" height=\"280\" />\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from fromage import models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the FROMAGe model used in the paper.\n",
    "model_dir = './fromage_model/'\n",
    "model = models.load_fromage(model_dir)\n",
    "\n",
    "# Load the all-MiniLM-L6-v2 model to compare the text embeddings of the output with those of the original caption.\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "lm = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dictionary(input_dict: dict, chunk_size: int) -> list:\n",
    "    res = []\n",
    "    new_dict = {}\n",
    "    for k, v in input_dict.items():\n",
    "        if len(new_dict) < chunk_size:\n",
    "            new_dict[k] = v\n",
    "        else:\n",
    "            res.append(new_dict)\n",
    "            new_dict = {k: v}\n",
    "    res.append(new_dict)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Flickr8k_text/ExpertAnnotations.txt',delimiter='\\t')\n",
    "cropped_df = df.loc[df['expert1'] == 4]\n",
    "cap_df = pd.read_csv('./Flickr8k_text/Flickr8k.token.txt',delimiter='\\t')\n",
    "cap_dict = pd.Series(cap_df.cap.values,index=cap_df.cap_id).to_dict()\n",
    "data_dict = {}\n",
    "for img_id, cap_id in zip(cropped_df.image_id, cropped_df.caption_id):\n",
    "    caption = cap_dict[cap_id]\n",
    "    data_dict[img_id] = caption\n",
    "    \n",
    "flickr_data = split_dictionary(data_dict,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions\n",
    "\n",
    "- The cos_sim computes function the cosine similarity. We need it to compare the text embeddings of the outputs of the model.\n",
    "- The mean_pooling function is used as an processing tool for the embeddings.\n",
    "- The display_interleaved_outputs function is used to plot the images generated by the model.\n",
    "- The compare_embeddings function uses the Mini-LM-L6 model to generate the text embeddings and then calls the mean_pooling function and the cos_sim function to provide a final score. The following image describes the aforementioned procedure.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images_report/embeds_cos_sim.png\" width=\"700\" height=\"200\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "\n",
    "def mean_pooling(model_output: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    token_embeddings = model_output[0] \n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def display_interleaved_outputs(model_outputs, one_img_per_ret=True):\n",
    "    for output in model_outputs:\n",
    "        if type(output) == str:\n",
    "            print(output)\n",
    "        elif type(output) == list:\n",
    "            if one_img_per_ret:\n",
    "                plt.figure(figsize=(3, 3))\n",
    "                plt.imshow(np.array(output[0]))\n",
    "            else:\n",
    "                fig, ax = plt.subplots(1, len(output), figsize=(3 * len(output), 3))\n",
    "                for i, image in enumerate(output):\n",
    "                    image = np.array(image)\n",
    "                    ax[i].imshow(image)\n",
    "                    ax[i].set_title(f'Retrieval #{i+1}')\n",
    "            plt.show()\n",
    "        elif type(output) == Image.Image:\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.imshow(np.array(output))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def compare_embeddings(unaugmented_input, augmented_input, answer):\n",
    "\n",
    "    # Generate captions using the unaugmented and the augmented input\n",
    "    unaugmented_output = model.generate_for_images_and_texts(unaugmented_input, num_words=15, max_num_rets=0)\n",
    "    augmented_output = model.generate_for_images_and_texts(augmented_input, num_words=15, max_num_rets=0)\n",
    "\n",
    "    # Tokenize the input\n",
    "    encoded_unaugmented_input = tokenizer(unaugmented_output, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_augmented_input = tokenizer(augmented_output, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_target_input = tokenizer(answer, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # FF through the model\n",
    "    with torch.no_grad():\n",
    "        model_unaugmented_output = lm(**encoded_unaugmented_input)\n",
    "        model_augmented_output = lm(**encoded_augmented_input)\n",
    "        model_target_output = lm(**encoded_target_input)\n",
    "\n",
    "    # Process the embeddings\n",
    "    unaugmented_embeddings = F.normalize(mean_pooling(model_unaugmented_output, encoded_unaugmented_input['attention_mask']), p=2, dim=1)\n",
    "    augmented_embeddings = F.normalize(mean_pooling(model_augmented_output, encoded_augmented_input['attention_mask']), p=2, dim=1)\n",
    "    target_embeddings = F.normalize(mean_pooling(model_target_output, encoded_target_input['attention_mask']), p=2, dim=1)\n",
    "\n",
    "    # Compute the cos sim \n",
    "    augmented_score = cos_sim(augmented_embeddings, target_embeddings)\n",
    "    unaugmented_score = cos_sim(unaugmented_embeddings, target_embeddings)\n",
    "\n",
    "    return augmented_score, unaugmented_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We obviously can not pass all the data through the model, because it needs time. So we will just pick 3 examples for the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr_images_folder = './Flicker8k_Dataset/'\n",
    "flickr_data = flickr_data[5:8] \n",
    "\n",
    "for flickr_dict in flickr_data:\n",
    "    try:\n",
    "\n",
    "        flickr_keys = list(flickr_dict.keys())\n",
    "        flickr_values = list(flickr_dict.values())\n",
    "\n",
    "        # Load query image & caption\n",
    "        question_image_path = flickr_keys[0]\n",
    "        question_image = Image.open(os.path.join(flickr_dict,question_image_path)).resize((224, 224)).convert('RGB')\n",
    "        print(\"The original query image is the following :\")\n",
    "        display_interleaved_outputs([question_image])\n",
    "        question = 'Caption the image.'\n",
    "        answer = flickr_values[0]\n",
    "\n",
    "        # Generate caption using the unaugmented prompt\n",
    "        unaugmented_prompt = [question_image,question]\n",
    "        unaugmented_output = model.generate_for_images_and_texts(unaugmented_prompt, num_words=15, max_num_rets=0)\n",
    "\n",
    "\n",
    "        # Use query image to retrieve two similar ones\n",
    "        prompt_for_ret = [question_image, 'Give a similar image to the previous one [RET]']\n",
    "        augmented_outputs = model.generate_for_images_and_texts(prompt_for_ret, max_img_per_ret=2) \n",
    "        for out in augmented_outputs:\n",
    "                if type(out) == str:\n",
    "                    continue\n",
    "                elif type(out) == list:\n",
    "                    similar_image1 = out[0]\n",
    "                    print(\"The first image similar to the query image retrieved by the model is the following :\")\n",
    "                    display_interleaved_outputs([similar_image1])\n",
    "                    similar_image2 = out[1]\n",
    "                    print(\"The second image similar to the query image retrieved by the model is the following :\")\n",
    "                    display_interleaved_outputs([similar_image2])\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        model_augmented_input = [similar_image1, similar_image2, question_image, question]\n",
    "        augmented_output = model.generate_for_images_and_texts(model_augmented_input, num_words=15, max_num_rets=0)\n",
    "\n",
    "        augmented_score, unaugmented_score = compare_embeddings(unaugmented_output, augmented_output, answer)\n",
    "\n",
    "        print(\"Caption without using any augmentation\", unaugmented_output, \"| Cos sim with target :\",unaugmented_score.item())\n",
    "        print(\"Caption using augmentation\", augmented_output, \"| Cos sim with target :\",augmented_score.item())\n",
    "        print(\"Ground truth :\", answer)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
