{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook is used to give you a demonstration of how image retrieval is performed using FROMAGe. Specifically, it aims to prove how in-context learning is applied for image retrieval using the Flickr-8k dataset and how a text augmentation can increase the model's performance for this downstream task. The image below describes the procedure in a more comprehensible way.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images_report/Text_augmentation_of_prompt.png\" width=\"720\" height=\"300\" />\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from fromage import models\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the FROMAGe model used in the paper.\n",
    "model_dir = './fromage_model/'\n",
    "model = models.load_fromage(model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dictionary(input_dict: dict, chunk_size: int) -> list:\n",
    "    res = []\n",
    "    new_dict = {}\n",
    "    for k, v in input_dict.items():\n",
    "        if len(new_dict) < chunk_size:\n",
    "            new_dict[k] = v\n",
    "        else:\n",
    "            res.append(new_dict)\n",
    "            new_dict = {k: v}\n",
    "    res.append(new_dict)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended caption for text augmentation\n",
    "extended_captions = open(\"extended_captions.txt\", \"r\")\n",
    "augmented_captions = [x.rstrip(\"\\n\") for x in extended_captions.readlines()]\n",
    "\n",
    "# Read the data for the image retrieval task\n",
    "df = pd.read_csv('./Flickr8k_text/ExpertAnnotations.txt',delimiter='\\t')\n",
    "cropped_df = df.loc[df['expert1'] == 4]\n",
    "img_cap_list = list(zip(cropped_df.image_id, cropped_df.caption_id))\n",
    "cap_df = pd.read_csv('./Flickr8k_text/Flickr8k.token.txt',delimiter='\\t')\n",
    "cap_dict = pd.Series(cap_df.cap.values,index=cap_df.cap_id).to_dict()\n",
    "data_dict = {}\n",
    "for img_id, cap_id in zip(cropped_df.image_id, cropped_df.caption_id):\n",
    "    caption = cap_dict[cap_id]\n",
    "    data_dict[img_id] = caption \n",
    "for i,content in enumerate(zip(data_dict.keys(),data_dict.values())):\n",
    "    data_dict[content[0]] = (content[1],augmented_captions[i])\n",
    "    \n",
    "ic_data = split_dictionary(data_dict,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions\n",
    "\n",
    "The display_interleaved_outputs function is used to plot the images generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_interleaved_outputs(model_outputs, one_img_per_ret=True):\n",
    "    for output in model_outputs:\n",
    "        if type(output) == str:\n",
    "            print(output)\n",
    "        elif type(output) == list:\n",
    "            if one_img_per_ret:\n",
    "                plt.figure(figsize=(3, 3))\n",
    "                plt.imshow(np.array(output[0]))\n",
    "            else:\n",
    "                fig, ax = plt.subplots(1, len(output), figsize=(3 * len(output), 3))\n",
    "                for i, image in enumerate(output):\n",
    "                    image = np.array(image)\n",
    "                    ax[i].imshow(image)\n",
    "                    ax[i].set_title(f'Retrieval #{i+1}')\n",
    "            plt.show()\n",
    "        elif type(output) == Image.Image:\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.imshow(np.array(output))\n",
    "            plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We obviously can not pass all the data through the model, because it needs time. So we will just pick 3 examples for the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_images_folder = './Flicker8k_Dataset/'\n",
    "ic_data = ic_data[5:8] \n",
    "\n",
    "for ic_dict in ic_data:\n",
    "\n",
    "    i+=1\n",
    "    image_path = list(ic_dict.keys())[0]\n",
    "    image = Image \\\n",
    "        .open(os.path.join('./Flicker8k_Dataset/',image_path)) \\\n",
    "        .resize((224, 224)) \\\n",
    "        .convert('RGB') \n",
    "    caption_tuple = list(ic_dict.values())[0]\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Retrieve image based on the original caption\n",
    "        original_caption = caption_tuple[0]\n",
    "        original_prompt = [original_caption[:-1] + ' [RET] ']\n",
    "        model_output_orig = model.generate_for_images_and_texts(original_prompt, max_img_per_ret=1, max_num_rets=1, num_words=0)\n",
    "        unaugmented_output = model_output_orig[-1][0]\n",
    "        print(\"The retrieved image without using any augmentation is the following :\")\n",
    "        display_interleaved_outputs(unaugmented_output)\n",
    "\n",
    "\n",
    "        # Retrieve image based on the augmented caption\n",
    "        augmented_caption = caption_tuple[1]\n",
    "        augmented_prompt = [image, augmented_caption[:-1] + ' [RET] ']\n",
    "        model_output = model.generate_for_images_and_texts(augmented_prompt, max_img_per_ret=1, max_num_rets=1, num_words=0)\n",
    "        augmented_output = model_output[-1][0]\n",
    "        print(\"The retrieved image using text augmentation is the following :\")\n",
    "        display_interleaved_outputs(augmented_output)\n",
    "\n",
    "        # Evaluation metric for the two retrieved images (with and without using text augmentation)\n",
    "        transform = ToTensor()\n",
    "        model = model.to(torch.float16)\n",
    "        with torch.no_grad():\n",
    "            embedding_augmented = model.model.get_visual_embs((transform(augmented_output)).unsqueeze(0).half().cuda())\n",
    "            embedding_unaugmented = model.model.get_visual_embs((transform(unaugmented_output)).unsqueeze(0).half().cuda())\n",
    "            embedding_original = model.model.get_visual_embs((transform(image)).unsqueeze(0).half().cuda())\n",
    "        model = model.bfloat16()\n",
    "        similarity_score_unaugmented = torch.nn.functional.cosine_similarity(embedding_unaugmented.float(), embedding_original.float())\n",
    "        similarity_score_augmented = torch.nn.functional.cosine_similarity(embedding_augmented.float(), embedding_original.float())\n",
    "\n",
    "        print(\"Similarity of Unaugmented - Original \",similarity_score_unaugmented.mean(1).item())\n",
    "        print(\"Similarity of Augmented - Original \",similarity_score_augmented.mean(1).item())\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
